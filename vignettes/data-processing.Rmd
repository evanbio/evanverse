---
title: "Data Processing and Transformation Mastery"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Processing and Transformation Mastery}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 10,
  fig.height = 6,
  dpi = 300,
  out.width = "100%"
)
```

```{r setup, message = FALSE}
library(evanverse)
library(dplyr)
library(tidyr)
library(ggplot2)
```

# ğŸ”„ Data Processing and Transformation Mastery

The `evanverse` package provides a comprehensive suite of data processing tools designed to handle the messy realities of real-world data. This guide covers void value handling, data transformation, custom operators, flexible file I/O, and advanced data manipulation techniques.

## ğŸ¯ Overview of Data Processing Functions

### Core Data Processing Arsenal

| Category | Functions | Purpose |
|----------|-----------|---------|
| **Void Handling** | `is_void()`, `any_void()`, `drop_void()`, `replace_void()` | Handle NA, NULL, empty values |
| **Data Structure** | `cols_with_void()`, `rows_with_void()` | Identify problematic data regions |
| **Transformation** | `df2list()`, `map_column()` | Restructure and remap data |
| **File I/O** | `read_table_flex()`, `read_excel_flex()`, `write_xlsx_flex()` | Robust file operations |
| **Custom Operators** | `%p%`, `%is%`, `%nin%`, `%match%`, `%map%` | Expressive data filtering |
| **Logic & Math** | `combine_logic()`, `comb()`, `perm()` | Advanced logical operations |

## ğŸ•³ï¸ Mastering Void Value Handling

### Understanding Void Values

"Void" values in evanverse represent various forms of missing or empty data that can disrupt analysis.

```{r void-concepts}
# Various types of void values in real data
messy_vector <- c(
  "valid_data",    # Normal value
  "",              # Empty string
  "   ",           # Whitespace only
  NA,              # R's missing value
  NULL,            # Null value
  "NA",            # String "NA"
  "NULL",          # String "NULL"
  "n/a",           # Common missing indicator
  "#N/A",          # Excel missing value
  0,               # Sometimes considered void in context
  "."              # SAS missing indicator
)

cat("ğŸ” Void Value Detection Results:\n")
cat("================================\n")
void_results <- data.frame(
  Value = sapply(messy_vector, function(x) if(is.null(x)) "NULL" else as.character(x)),
  Is_Void = is_void(messy_vector),
  stringsAsFactors = FALSE
)

print(void_results)

cat("\nVoid value summary:\n")
cat("Total values:", length(messy_vector), "\n")
cat("Void values:", sum(is_void(messy_vector), na.rm = TRUE), "\n")
cat("Valid values:", sum(!is_void(messy_vector), na.rm = TRUE), "\n")
```

### Comprehensive Void Value Operations

```{r void-operations}
# Create a realistic messy dataset
set.seed(123)
messy_data <- data.frame(
  ID = 1:12,
  Name = c("Alice", "Bob", "", "Diana", "NA", "Frank", "   ", "Helen",
           "Ivan", "NULL", "Kate", "Liam"),
  Age = c(25, NA, 30, 0, 28, 35, 22, NA, 40, 45, NA, 33),
  Salary = c(50000, 60000, NA, "", 70000, "n/a", 80000, 90000,
             NULL, 100000, ".", 110000),
  Department = c("HR", "IT", "Finance", "", "IT", "HR", "Marketing",
                "Finance", "IT", NA, "HR", "   "),
  stringsAsFactors = FALSE
)

cat("ğŸ“Š Original Messy Dataset:\n")
cat("==========================\n")
print(messy_data)

# Identify void values by column
cat("\nğŸ” Void Value Analysis by Column:\n")
cat("=================================\n")
void_by_column <- sapply(messy_data, function(col) sum(is_void(col), na.rm = TRUE))
print(void_by_column)

# Identify rows with void values
void_rows <- rows_with_void(messy_data)
cat("\nRows containing void values:", paste(void_rows, collapse = ", "), "\n")

# Identify columns with void values
void_cols <- cols_with_void(messy_data)
cat("Columns containing void values:", paste(void_cols, collapse = ", "), "\n")

# Check if any void values exist
has_voids <- any_void(messy_data)
cat("Dataset contains void values:", has_voids, "\n")
```

### Void Value Replacement Strategies

```{r void-replacement}
# Strategy 1: Replace with specific values
data_replaced_specific <- messy_data
data_replaced_specific$Name <- replace_void(data_replaced_specific$Name, "Unknown")
data_replaced_specific$Age <- replace_void(data_replaced_specific$Age, median(as.numeric(messy_data$Age), na.rm = TRUE))
data_replaced_specific$Department <- replace_void(data_replaced_specific$Department, "Unassigned")

cat("ğŸ“‹ Strategy 1: Replace with Specific Values\n")
cat("===========================================\n")
print(data_replaced_specific[1:6, c("Name", "Age", "Department")])

# Strategy 2: Replace with contextual values
data_replaced_contextual <- messy_data

# Replace void salaries with department median
dept_medians <- data_replaced_contextual %>%
  mutate(Salary = as.numeric(replace_void(Salary, "0"))) %>%
  filter(!is_void(Department)) %>%
  group_by(Department) %>%
  summarise(Median_Salary = median(Salary, na.rm = TRUE), .groups = 'drop')

cat("\nğŸ’° Department Salary Medians for Replacement:\n")
print(dept_medians)

# Strategy 3: Drop void values
data_dropped <- drop_void(messy_data, method = "rows")
cat("\nğŸ—‘ï¸ Strategy 3: After Dropping Rows with Void Values\n")
cat("===================================================\n")
cat("Original rows:", nrow(messy_data), "\n")
cat("Remaining rows:", nrow(data_dropped), "\n")
print(head(data_dropped))
```

### Advanced Void Handling Techniques

```{r advanced-void-handling, fig.cap="Visualization of void value patterns across dataset"}
# Create void pattern visualization
void_pattern <- messy_data %>%
  mutate(across(everything(), is_void)) %>%
  mutate(Row = row_number()) %>%
  pivot_longer(cols = -Row, names_to = "Column", values_to = "Is_Void")

# Plot void patterns
p1 <- ggplot(void_pattern, aes(x = Column, y = as.factor(Row), fill = Is_Void)) +
  geom_tile(color = "white", size = 0.5) +
  scale_fill_manual(
    values = c("FALSE" = get_palette("vividset", type = "qualitative", n = 2)[2],
               "TRUE" = get_palette("vividset", type = "qualitative", n = 2)[1]),
    name = "Void Value",
    labels = c("Valid", "Void")
  ) +
  labs(
    title = "Void Value Pattern Analysis",
    subtitle = "Identifying missing data patterns across rows and columns",
    x = "Column",
    y = "Row"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", color = "#0D47A1"),
    plot.subtitle = element_text(size = 11, color = "#666666"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )

print(p1)

# Void completeness analysis
completeness_stats <- messy_data %>%
  summarise(across(everything(), ~ sum(!is_void(.)) / length(.) * 100)) %>%
  pivot_longer(everything(), names_to = "Column", values_to = "Completeness_Percent")

cat("\nğŸ“Š Data Completeness by Column:\n")
print(completeness_stats)
```

## ğŸ”„ Data Structure Transformation

### DataFrame to List Conversion

```{r df2list-demo}
# Create sample data for transformation
pathway_data <- data.frame(
  Pathway = c("Glycolysis", "Glycolysis", "Glycolysis", "TCA_Cycle",
              "TCA_Cycle", "Fatty_Acid", "Fatty_Acid", "Fatty_Acid"),
  Gene = c("HK1", "PFKL", "LDHA", "CS", "IDH1", "FASN", "ACACA", "SCD"),
  Function = c("Glucose_phosphorylation", "Rate_limiting", "Lactate_production",
               "Acetyl_CoA_condensation", "NADH_generation", "Fatty_acid_synthesis",
               "Acetyl_CoA_carboxylase", "Desaturation"),
  Expression = c(5.2, 7.8, 6.1, 4.5, 3.9, 8.2, 6.7, 5.8),
  stringsAsFactors = FALSE
)

cat("ğŸ“Š Original Pathway Data:\n")
print(pathway_data)

# Convert to pathway-gene lists
pathway_gene_list <- df2list(pathway_data, key_col = "Pathway", value_cols = "Gene")
cat("\nğŸ”„ Converted to Pathway-Gene Lists:\n")
print(pathway_gene_list)

# Convert to pathway-function mapping
pathway_function_list <- df2list(pathway_data, key_col = "Pathway", value_cols = "Function")
cat("\nğŸ“‹ Pathway-Function Mapping:\n")
str(pathway_function_list)

# Multiple value columns
pathway_multi_list <- df2list(pathway_data,
                             key_col = "Pathway",
                             value_cols = c("Gene", "Function"))
cat("\nğŸ”— Multi-column Transformation:\n")
str(pathway_multi_list)
```

### Column Mapping and Transformation

```{r column-mapping}
# Create data with inconsistent column names (common real-world problem)
inconsistent_data <- data.frame(
  `Patient ID` = paste0("P", 1:8),
  `patient_age` = c(45, 52, 38, 61, 29, 47, 55, 33),
  `Gender_M_F` = c("M", "F", "F", "M", "F", "M", "M", "F"),
  `BloodPressure_systolic` = c(120, 130, 115, 140, 110, 125, 135, 118),
  `bp_diastolic` = c(80, 85, 75, 90, 70, 82, 88, 76),
  check.names = FALSE,
  stringsAsFactors = FALSE
)

cat("ğŸ“‹ Data with Inconsistent Column Names:\n")
cat("=======================================\n")
print(inconsistent_data)

# Create standardized mapping
column_mapping <- c(
  "Patient ID" = "patient_id",
  "patient_age" = "age",
  "Gender_M_F" = "gender",
  "BloodPressure_systolic" = "systolic_bp",
  "bp_diastolic" = "diastolic_bp"
)

cat("\nğŸ—ºï¸ Column Name Mapping:\n")
print(data.frame(Original = names(column_mapping),
                 Standardized = column_mapping,
                 stringsAsFactors = FALSE))

# Apply mapping
standardized_data <- map_column(inconsistent_data, mapping = column_mapping)
cat("\nâœ¨ Standardized Dataset:\n")
print(standardized_data)

# Verify transformation
cat("\nColumn name transformation verification:\n")
cat("Original:", paste(names(inconsistent_data), collapse = ", "), "\n")
cat("Standardized:", paste(names(standardized_data), collapse = ", "), "\n")
```

## âš¡ Custom Operators for Expressive Code

### String Operations with %p%

```{r string-operators}
# String concatenation with space
greeting <- "Hello" %p% "World" %p% "from" %p% "evanverse!"
cat("Basic concatenation:", greeting, "\n")

# Building file paths
data_path <- "analysis" %p% "results" %p% "2024" %p% "experiment_1.csv"
cat("File path building:", data_path, "\n")

# Creating informative labels
conditions <- c("Control", "Treatment_A", "Treatment_B")
sample_sizes <- c(25, 30, 28)
labels <- conditions %p% "(n=" %p% sample_sizes %p% ")"
cat("Dynamic labels:\n")
for(i in seq_along(labels)) {
  cat("  ", labels[i], "\n")
}

# SQL-like query building
table_name <- "patients"
condition <- "age > 65"
query <- "SELECT * FROM" %p% table_name %p% "WHERE" %p% condition
cat("SQL query:", query, "\n")
```

### Pattern Matching with %is% and %nin%

```{r pattern-operators}
# Pattern matching examples
text_samples <- c("RNA-seq", "ChIP-seq", "ATAC-seq", "Hi-C", "microarray")

# Check for sequencing methods
seq_methods <- text_samples[text_samples %is% "seq"]
cat("Sequencing methods:", paste(seq_methods, collapse = ", "), "\n")

# Case-insensitive matching
rna_methods <- text_samples[text_samples %is% "rna"]
cat("RNA-related methods:", paste(rna_methods, collapse = ", "), "\n")

# Not-in operator
gene_list <- c("BRCA1", "TP53", "EGFR", "MYC", "KRAS")
exclude_genes <- c("TP53", "MYC")
filtered_genes <- gene_list[gene_list %nin% exclude_genes]
cat("Filtered gene list:", paste(filtered_genes, collapse = ", "), "\n")

# Practical filtering example
sample_data <- data.frame(
  Sample_ID = paste0("S", 1:10),
  Tissue = c("brain", "liver", "heart", "brain", "kidney",
             "liver", "heart", "lung", "brain", "kidney"),
  Quality = c("high", "medium", "high", "low", "high",
              "medium", "high", "high", "medium", "low"),
  stringsAsFactors = FALSE
)

# Filter for brain samples with high quality
brain_high <- sample_data[sample_data$Tissue %is% "brain" &
                         sample_data$Quality %is% "high", ]
cat("\nHigh-quality brain samples:\n")
print(brain_high)

# Exclude low quality samples
good_samples <- sample_data[sample_data$Quality %nin% "low", ]
cat("\nSamples excluding low quality:\n")
print(good_samples[, c("Sample_ID", "Quality")])
```

### Advanced Matching with %match% and %map%

```{r advanced-operators}
# %match% for flexible matching
gene_symbols <- c("BRCA1", "BRCA2", "TP53", "EGFR")
search_terms <- c("brc", "p53", "egf")

# Find genes matching patterns
matching_genes <- lapply(search_terms, function(term) {
  gene_symbols[gene_symbols %match% term]
})
names(matching_genes) <- search_terms

cat("ğŸ” Pattern Matching Results:\n")
str(matching_genes)

# %map% for value mapping
old_labels <- c("treatment_1", "treatment_2", "control", "treatment_1", "control")
label_mapping <- c("treatment_1" = "Drug_A",
                   "treatment_2" = "Drug_B",
                   "control" = "Placebo")

new_labels <- old_labels %map% label_mapping
cat("\nğŸ—ºï¸ Label Mapping:\n")
print(data.frame(Original = old_labels, Mapped = new_labels))

# Conditional mapping with functions
dose_values <- c(0, 5, 10, 25, 50, 100)
dose_categories <- dose_values %map% function(x) {
  if (x == 0) return("Control")
  if (x < 10) return("Low")
  if (x < 50) return("Medium")
  return("High")
}

cat("\nDose categorization:\n")
print(data.frame(Dose = dose_values, Category = dose_categories))
```

## ğŸ“ Flexible File I/O Operations

### Advanced Table Reading

```{r file-operations, eval = FALSE}
# Demonstrate flexible table reading capabilities
# Note: These examples show function usage patterns

# Auto-detect delimiter and handle various formats
data1 <- read_table_flex("data.csv")           # Comma-separated
data2 <- read_table_flex("data.tsv")           # Tab-separated
data3 <- read_table_flex("data.txt", sep = "|") # Pipe-separated
data4 <- read_table_flex("data.csv", skip = 3) # Skip header lines

# Handle problematic encodings
data5 <- read_table_flex("international_data.csv", encoding = "UTF-8")

# Read with custom column types
data6 <- read_table_flex("mixed_data.csv",
                        col_types = list(ID = "character",
                                       Date = "Date",
                                       Value = "numeric"))
```

```{r file-demo}
# Create sample data to demonstrate file operations
sample_dataset <- data.frame(
  Patient_ID = paste0("P", str_pad(1:50, 3, pad = "0")),
  Age = sample(25:80, 50, replace = TRUE),
  Gender = sample(c("Male", "Female"), 50, replace = TRUE),
  Treatment = sample(c("Drug_A", "Drug_B", "Placebo"), 50, replace = TRUE),
  Response = round(rnorm(50, 75, 15), 1),
  Visit_Date = seq.Date(from = as.Date("2024-01-01"),
                       by = "week", length.out = 50),
  stringsAsFactors = FALSE
)

cat("ğŸ“Š Sample Clinical Dataset (first 6 rows):\n")
print(head(sample_dataset))

cat("\nğŸ“‹ Dataset Summary:\n")
cat("Total patients:", nrow(sample_dataset), "\n")
cat("Variables:", ncol(sample_dataset), "\n")
cat("Date range:", as.character(range(sample_dataset$Visit_Date)), "\n")
cat("Treatments:", paste(unique(sample_dataset$Treatment), collapse = ", "), "\n")
```

### Excel File Operations

```{r excel-operations, eval = FALSE}
# Advanced Excel operations
# Note: These demonstrate function capabilities

# Read specific sheets and ranges
excel_data1 <- read_excel_flex("clinical_data.xlsx",
                               sheet = "Patient_Demographics")

excel_data2 <- read_excel_flex("lab_results.xlsx",
                               sheet = 2,
                               range = "A1:F100",
                               skip = 1)

# Write multiple sheets to Excel
dataset_list <- list(
  Demographics = sample_dataset[, c("Patient_ID", "Age", "Gender")],
  Treatments = sample_dataset[, c("Patient_ID", "Treatment", "Response")],
  Visits = sample_dataset[, c("Patient_ID", "Visit_Date")]
)

write_xlsx_flex(dataset_list,
                filename = "clinical_analysis.xlsx",
                row_names = FALSE)
```

## ğŸ”¢ Advanced Logic and Mathematical Operations

### Logical Operations

```{r logic-operations}
# Combine multiple logical conditions
condition1 <- c(TRUE, FALSE, TRUE, FALSE, TRUE)
condition2 <- c(TRUE, TRUE, FALSE, FALSE, TRUE)
condition3 <- c(FALSE, TRUE, TRUE, FALSE, TRUE)

# Different combination methods
combined_and <- combine_logic(condition1, condition2, method = "and")
combined_or <- combine_logic(condition1, condition2, method = "or")

cat("ğŸ”— Logical Operations:\n")
cat("=====================\n")
logic_table <- data.frame(
  Condition1 = condition1,
  Condition2 = condition2,
  Combined_AND = combined_and,
  Combined_OR = combined_or
)
print(logic_table)

# Practical application: filtering data
clinical_data <- data.frame(
  Patient = paste0("P", 1:8),
  Age_Over_60 = c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE),
  Has_Comorbidity = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE),
  High_Risk_Gene = c(FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE),
  stringsAsFactors = FALSE
)

# Define high-risk patients using logical combinations
clinical_data$High_Risk <- combine_logic(
  clinical_data$Age_Over_60,
  clinical_data$Has_Comorbidity,
  method = "and"
) | clinical_data$High_Risk_Gene

cat("\nğŸ¥ Clinical Risk Assessment:\n")
print(clinical_data)
```

### Combinatorial Analysis

```{r combinatorics}
# Combinatorial calculations for experimental design
total_samples <- 12
samples_per_group <- 3

# Calculate combinations and permutations
combinations_result <- comb(n = total_samples, k = samples_per_group)
permutations_result <- perm(n = total_samples, k = samples_per_group)

cat("ğŸ§® Combinatorial Analysis:\n")
cat("=========================\n")
cat("Total samples:", total_samples, "\n")
cat("Samples per group:", samples_per_group, "\n")
cat("Possible combinations (order doesn't matter):", combinations_result, "\n")
cat("Possible permutations (order matters):", permutations_result, "\n")

# Practical application: experimental design
treatments <- c("Control", "Drug_A", "Drug_B", "Drug_C")
n_treatments <- length(treatments)

# How many ways to select 2 treatments for comparison?
pairwise_comparisons <- comb(n = n_treatments, k = 2)
cat("\nExperimental Design:\n")
cat("Available treatments:", paste(treatments, collapse = ", "), "\n")
cat("Possible pairwise comparisons:", pairwise_comparisons, "\n")

# Generate all pairwise combinations
treatment_pairs <- combn(treatments, 2, simplify = FALSE)
cat("All pairwise combinations:\n")
for(i in seq_along(treatment_pairs)) {
  cat("  ", paste(treatment_pairs[[i]], collapse = " vs "), "\n")
}
```

## ğŸ“Š Real-World Data Processing Workflows

### Complete Data Cleaning Pipeline

```{r cleaning-pipeline, fig.cap="Before and after comparison of data cleaning pipeline"}
# Create realistic messy biomedical data
set.seed(456)
messy_biodata <- data.frame(
  sample_id = c("S001", "S002", "", "S004", "NA", "S006", "   ", "S008"),
  patient_age = c("45", "52.5", "NA", "61", "", "47", "UNKNOWN", "33"),
  gender = c("M", "F", "f", "MALE", "Female", "M", "", "F"),
  biomarker_1 = c(5.2, NA, 7.8, 0, 6.1, "n/a", 4.5, 8.9),
  biomarker_2 = c("3.1", "4.2", "", "5.8", "NULL", "2.9", "6.7", "3.5"),
  treatment_group = c("control", "drug_a", "CONTROL", "drug_a", "Drug_B",
                     "control", "drug_b", "DRUG_A"),
  response_score = c(78.5, "", 85.2, "missing", 92.1, 67.8, "N/A", 89.3),
  stringsAsFactors = FALSE
)

cat("ğŸ”§ Data Cleaning Pipeline\n")
cat("=========================\n")
cat("Step 1: Original Messy Data\n")
print(messy_biodata)

# Step 1: Handle void values in ID column
cleaned_data <- messy_biodata
cleaned_data$sample_id <- replace_void(cleaned_data$sample_id, "UNKNOWN")

# Step 2: Clean and standardize age
cleaned_data$patient_age <- as.numeric(replace_void(cleaned_data$patient_age, NA))
median_age <- median(cleaned_data$patient_age, na.rm = TRUE)
cleaned_data$patient_age[is.na(cleaned_data$patient_age)] <- median_age

# Step 3: Standardize gender values
gender_mapping <- c("M" = "Male", "F" = "Female", "f" = "Female",
                   "MALE" = "Male", "Female" = "Female")
cleaned_data$gender <- replace_void(cleaned_data$gender, "Unknown")
cleaned_data$gender <- cleaned_data$gender %map% gender_mapping

# Step 4: Handle biomarkers
cleaned_data$biomarker_1 <- as.numeric(replace_void(cleaned_data$biomarker_1, NA))
cleaned_data$biomarker_2 <- as.numeric(replace_void(cleaned_data$biomarker_2, NA))

# Step 5: Standardize treatment groups
treatment_mapping <- c("control" = "Control", "CONTROL" = "Control",
                      "drug_a" = "Drug_A", "DRUG_A" = "Drug_A",
                      "drug_b" = "Drug_B", "Drug_B" = "Drug_B")
cleaned_data$treatment_group <- cleaned_data$treatment_group %map% treatment_mapping

# Step 6: Clean response scores
cleaned_data$response_score <- as.numeric(replace_void(cleaned_data$response_score, NA))

cat("\nStep 2: Cleaned Data\n")
print(cleaned_data)

# Data quality comparison
original_void_count <- sum(sapply(messy_biodata, function(col) sum(is_void(col), na.rm = TRUE)))
cleaned_void_count <- sum(sapply(cleaned_data, function(col) sum(is.na(col))))

cat("\nğŸ“Š Cleaning Results:\n")
cat("Original void values:", original_void_count, "\n")
cat("Remaining missing values:", cleaned_void_count, "\n")
cat("Data quality improvement:",
    round((original_void_count - cleaned_void_count) / original_void_count * 100, 1), "%\n")
```

### Multi-step Data Transformation

```{r transformation-workflow}
# Complex transformation workflow
# Simulate gene expression data with metadata
expression_raw <- data.frame(
  Gene_ID = paste0("ENSG", str_pad(1:20, 8, pad = "0")),
  Gene_Symbol = c("BRCA1", "TP53", "EGFR", "MYC", "KRAS", "PIK3CA", "AKT1",
                  "PTEN", "RB1", "CDKN2A", "MDM2", "ERBB2", "BRAF", "NRAS",
                  "FGFR1", "PDGFRA", "KIT", "ALK", "RET", "ROS1"),
  Control_1 = round(rnorm(20, 50, 15), 2),
  Control_2 = round(rnorm(20, 48, 12), 2),
  Treatment_1 = round(rnorm(20, 65, 18), 2),
  Treatment_2 = round(rnorm(20, 62, 16), 2),
  stringsAsFactors = FALSE
)

cat("ğŸ§¬ Gene Expression Transformation Workflow\n")
cat("==========================================\n")
cat("Original data structure:\n")
print(head(expression_raw))

# Step 1: Reshape from wide to long format
expression_long <- expression_raw %>%
  pivot_longer(cols = starts_with(c("Control", "Treatment")),
               names_to = "Sample", values_to = "Expression") %>%
  separate(Sample, into = c("Group", "Replicate"), sep = "_") %>%
  mutate(Condition = paste(Group, Replicate, sep = "_"))

cat("\nStep 1: Reshaped to long format (first 10 rows):\n")
print(head(expression_long, 10))

# Step 2: Calculate group statistics
expression_stats <- expression_long %>%
  group_by(Gene_Symbol, Group) %>%
  summarise(
    Mean_Expression = round(mean(Expression, na.rm = TRUE), 2),
    SD_Expression = round(sd(Expression, na.rm = TRUE), 2),
    .groups = 'drop'
  ) %>%
  pivot_wider(names_from = Group,
              values_from = c(Mean_Expression, SD_Expression),
              names_sep = "_")

cat("\nStep 2: Group statistics (first 8 genes):\n")
print(head(expression_stats, 8))

# Step 3: Calculate fold changes
expression_stats$Log2_Fold_Change <- round(
  log2((expression_stats$Mean_Expression_Treatment + 1) /
       (expression_stats$Mean_Expression_Control + 1)), 2)

expression_stats$Regulation <- ifelse(
  expression_stats$Log2_Fold_Change > 1, "Up",
  ifelse(expression_stats$Log2_Fold_Change < -1, "Down", "Unchanged"))

cat("\nStep 3: Fold change analysis:\n")
regulation_summary <- table(expression_stats$Regulation)
print(regulation_summary)

cat("\nTop regulated genes:\n")
top_regulated <- expression_stats %>%
  arrange(desc(abs(Log2_Fold_Change))) %>%
  select(Gene_Symbol, Log2_Fold_Change, Regulation) %>%
  head(8)
print(top_regulated)
```

## ğŸ›¡ï¸ Data Validation and Quality Control

### Comprehensive Data Validation

```{r data-validation}
# Create validation functions for common data quality checks
validate_dataset <- function(data, id_col = NULL, required_cols = NULL) {
  results <- list()

  # Basic structure checks
  results$n_rows <- nrow(data)
  results$n_cols <- ncol(data)
  results$col_names <- names(data)

  # Missing data analysis
  results$missing_by_col <- sapply(data, function(col) sum(is.na(col) | is_void(col)))
  results$missing_by_row <- rowSums(is.na(data) | sapply(data, is_void))
  results$complete_cases <- sum(complete.cases(data))

  # ID validation if specified
  if (!is.null(id_col) && id_col %in% names(data)) {
    results$duplicate_ids <- sum(duplicated(data[[id_col]]))
    results$missing_ids <- sum(is.na(data[[id_col]]) | is_void(data[[id_col]]))
  }

  # Required columns check
  if (!is.null(required_cols)) {
    results$missing_required_cols <- required_cols[!required_cols %in% names(data)]
  }

  return(results)
}

# Apply validation to cleaned dataset
validation_results <- validate_dataset(
  cleaned_data,
  id_col = "sample_id",
  required_cols = c("patient_age", "gender", "treatment_group")
)

cat("ğŸ” Data Validation Report\n")
cat("=========================\n")
cat("Dataset dimensions:", validation_results$n_rows, "rows Ã—",
    validation_results$n_cols, "columns\n")
cat("Complete cases:", validation_results$complete_cases,
    "(", round(validation_results$complete_cases / validation_results$n_rows * 100, 1), "%)\n")
cat("Duplicate IDs:", validation_results$duplicate_ids, "\n")
cat("Missing IDs:", validation_results$missing_ids, "\n")

cat("\nMissing data by column:\n")
missing_summary <- data.frame(
  Column = names(validation_results$missing_by_col),
  Missing_Count = validation_results$missing_by_col,
  Missing_Percent = round(validation_results$missing_by_col / validation_results$n_rows * 100, 1)
)
print(missing_summary)

# Data type validation
cat("\nData type validation:\n")
type_info <- sapply(cleaned_data, class)
expected_types <- c("sample_id" = "character", "patient_age" = "numeric",
                   "gender" = "character", "biomarker_1" = "numeric",
                   "biomarker_2" = "numeric", "treatment_group" = "character",
                   "response_score" = "numeric")

type_check <- data.frame(
  Column = names(type_info),
  Actual_Type = type_info,
  Expected_Type = expected_types[names(type_info)],
  Type_Match = type_info == expected_types[names(type_info)]
)
print(type_check)
```

## ğŸ“‹ Best Practices and Performance Tips

### Data Processing Best Practices

```{r best-practices}
cat("ğŸ† DATA PROCESSING BEST PRACTICES\n")
cat("=================================\n\n")

cat("ğŸ“Š Data Quality:\n")
cat("  â€¢ Always validate data before processing\n")
cat("  â€¢ Document all transformations and assumptions\n")
cat("  â€¢ Keep original data unchanged, work on copies\n")
cat("  â€¢ Use consistent missing value representations\n\n")

cat("ğŸ”„ Transformation Strategy:\n")
cat("  â€¢ Plan transformation pipeline before coding\n")
cat("  â€¢ Test each step with small data samples\n")
cat("  â€¢ Implement intermediate data checks\n")
cat("  â€¢ Use version control for transformation scripts\n\n")

cat("âš¡ Performance Optimization:\n")
cat("  â€¢ Use vectorized operations when possible\n")
cat("  â€¢ Cache frequently used transformations\n")
cat("  â€¢ Profile code to identify bottlenecks\n")
cat("  â€¢ Consider data.table for large datasets\n\n")

cat("ğŸ›¡ï¸ Error Prevention:\n")
cat("  â€¢ Validate inputs at function entry points\n")
cat("  â€¢ Use informative error messages\n")
cat("  â€¢ Implement graceful handling of edge cases\n")
cat("  â€¢ Test with various data scenarios\n")
```

### Performance Comparison

```{r performance-demo}
# Demonstrate performance considerations
large_data <- data.frame(
  ID = 1:10000,
  Value1 = rnorm(10000),
  Value2 = sample(c("A", "B", "C", "", NA), 10000, replace = TRUE),
  Value3 = rpois(10000, 5),
  stringsAsFactors = FALSE
)

cat("âš¡ Performance Optimization Examples\n")
cat("===================================\n")

# Example 1: Efficient void detection
system.time(result1 <- sapply(large_data, function(col) sum(is_void(col), na.rm = TRUE)))
cat("Void detection for", nrow(large_data), "rows completed\n")

# Example 2: Batch operations vs. row-by-row
cat("\nBatch operations are more efficient than loops:\n")
cat("âœ… Good: vectorized operations\n")
cat("âŒ Avoid: for loops on large datasets\n")

# Memory usage considerations
object_sizes <- sapply(list(large_data, cleaned_data, expression_raw),
                      function(x) format(object.size(x), units = "Kb"))
names(object_sizes) <- c("Large Dataset", "Cleaned Data", "Expression Data")

cat("\nMemory usage examples:\n")
print(object_sizes)
```

## ğŸ¯ Summary and Advanced Techniques

The evanverse data processing toolkit provides:

âœ… **Comprehensive void handling** with multiple strategies
âœ… **Flexible data transformation** for complex restructuring
âœ… **Expressive custom operators** for readable code
âœ… **Robust file I/O operations** with error handling
âœ… **Advanced logical operations** for complex filtering
âœ… **Quality validation tools** for reliable analysis

### Continue Learning:

- ğŸ“Š [Visualization Showcase](visualization-showcase.html) - Advanced plotting with clean data
- ğŸ§¬ [Bioinformatics Workflows](bioinformatics-workflows.html) - Domain-specific processing
- ğŸ¨ [Color Palette Guide](color-palettes.html) - Visual representation of processed data

### Essential Data Processing Functions:

```{r processing-quick-ref, eval = FALSE}
# Void value handling
is_void(data)                              # Detect void values
replace_void(data, replacement = "Unknown") # Replace void values
drop_void(data, method = "rows")           # Remove void data

# Data transformation
df2list(data, key_col = "Group")           # Convert to named lists
map_column(data, mapping = name_map)       # Rename columns

# Custom operators
text %p% "suffix"                          # String concatenation
values %nin% exclude_list                  # Not-in filtering
old_values %map% value_mapping             # Value mapping

# File operations
read_table_flex("data.csv", skip = 2)      # Flexible reading
write_xlsx_flex(data_list, "output.xlsx")  # Multi-sheet Excel
```

---

*ğŸ”„ Transform messy data into analysis-ready datasets with evanverse!*